{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pickle\n",
    "\n",
    "from pandas.core import datetools\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "The following code imports and validates the LendingClub data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read from pickle\n",
      "(1873317, 153)\n"
     ]
    }
   ],
   "source": [
    "# Imports loan data\n",
    "\n",
    "df = pd.DataFrame()\n",
    "basepath='./Source Data/Loan Data/'\n",
    "files = os.listdir(basepath)\n",
    "csvs = []\n",
    "\n",
    "for file in files:\n",
    "    if re.match('.*csv$',file):\n",
    "        csvs += [file]\n",
    "\n",
    "try:\n",
    "    #ignore this - was trying to pickle the data into\n",
    "    #formats like feather, hdf5, native python pickling, etc\n",
    "    # but found issues on python 3.7\n",
    "    df=pickle.load(open('full_data.p', 'rb'))\n",
    "    print('Read from pickle')\n",
    "except:\n",
    "    dates = [\n",
    "        'next_pymnt_d',\n",
    "        'hardship_start_date',\n",
    "        'hardship_end_date',\n",
    "        'payment_plan_start_date',\n",
    "        'earliest_cr_line',\n",
    "        'issue_d',\n",
    "        'debt_settlement_flag_date',\n",
    "        'last_pymnt_d'\n",
    "\n",
    "    ]\n",
    "    cols = df.dtypes\n",
    "    for csv in csvs:\n",
    "        path = basepath + csv\n",
    "        print(\"Reading\",path)\n",
    "        tdf = pd.read_csv(path,header=1,parse_dates=dates,low_memory=False)\n",
    "        df=df.append(tdf)\n",
    "    df.reset_index(inplace=True) # This will help with joining back data if necessary.\n",
    "    # Convert dates to datetime\n",
    "    #df['issue_d'] = pd.to_datetime(df['issue_d'])\n",
    "    #df['debt_settlement_flag_date'] = pd.to_datetime(df['debt_settlement_flag_date'])\n",
    "    df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'])\n",
    "    \n",
    "    #determine age of credit line prior to loan issue and convert to integer\n",
    "    # days of credit history\n",
    "    df['earliest_cr_line'] = (df['issue_d']-df['earliest_cr_line']).dt.days\n",
    "    \n",
    "    # convert issue_d to a year to consider economic conditions\n",
    "    #SHOULD WE GO TO QUARTERS?\n",
    "    #df['issue_d'] = df['issue_d'].dt.year\n",
    "    \n",
    "    df['duration'] = (df['last_pymnt_d'] - df['issue_d']).dt.days / 30\n",
    "    pickle.dump( df, open( \"full_data.p\", \"wb\" ) )\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1020552, 70)\n"
     ]
    }
   ],
   "source": [
    "# Limit to loans that are paid in full or written off. Uses dates so that \n",
    "# loans that are delinquent are not disproportionaltely dropped from data\n",
    "\n",
    "mature_filter = (df['loan_status']=='Fully Paid')|(df['loan_status']=='Charged Off')\n",
    "reduced_df = df[mature_filter] # Pulls only loans that are charged off or paid in full.\n",
    "#\n",
    "## Use my documentation to filter to only useful regressors\n",
    "data_dict = pd.read_excel('./Source Data/LCDataDictionary.xlsx',sheet_name='LoanStats')\n",
    "features = list(data_dict[data_dict['Useful Predictor']=='Yes']['LoanStatNew'].values)+['duration']\n",
    "reduced_df=reduced_df[features]\n",
    "\n",
    "# Combines fields when necessary\n",
    "reduced_df['fico_est'] = (reduced_df['fico_range_low']+reduced_df['fico_range_high'])/2\n",
    "\n",
    "reduced_df.drop(columns=['fico_range_low','fico_range_high'],inplace=True)\n",
    "print(reduced_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1020552, 69)\n"
     ]
    }
   ],
   "source": [
    "# Convert strings to numbers emp_length, int_rate, revol_util\n",
    "emp_length_map={'10+ years':10, '< 1 year':0, '1 year':1, '3 years':3, '8 years':8, '9 years':9,\n",
    "                '4 years':4, '5 years':5, '6 years':6, '2 years':2, '7 years':7}\n",
    "\n",
    "reduced_df['emp_length']=reduced_df['emp_length'].replace(pd.Series(emp_length_map))\n",
    "\n",
    "grade_map={\"A\":1,\"B\":2,\"C\":3,\"D\":4,\"E\":5,\"F\":6,\"G\":7}\n",
    "reduced_df['grade']=reduced_df['grade'].replace(pd.Series(grade_map))\n",
    "\n",
    "reduced_df['int_rate']=reduced_df['int_rate'].apply(lambda x: float(x[:-1]))\n",
    "reduced_df['revol_util']=reduced_df['revol_util'].apply(lambda x:\n",
    "                                                        x[:-1] if isinstance(x, str) else np.nan).astype(float)\n",
    "\n",
    "reduced_df['earliest_cr_line']=reduced_df['earliest_cr_line'].apply(lambda x:\n",
    "                                                        0.0 if np.isnan(x) else x)\n",
    "\n",
    "reduced_df.drop(columns=['emp_title'],inplace=True)\n",
    "\n",
    "print(reduced_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'application_type_Joint App', 'verification_status_Verified', 'purpose_major_purchase', 'purpose_other', 'purpose_medical', 'verification_status_Source Verified', 'purpose_house', 'term_ 60 months', 'purpose_home_improvement', 'purpose_renewable_energy', 'purpose_debt_consolidation', 'purpose_small_business'}\n",
      "(1020552, 77)\n"
     ]
    }
   ],
   "source": [
    "seta=set(reduced_df.columns)\n",
    "\n",
    "\n",
    "# 8/8 consolidated purpose\n",
    "# Consolidated where logical\n",
    "reduced_df['purpose'].replace('credit_card','debt_consolidation',inplace=True)\n",
    "reduced_df['purpose'].replace('educational','other',inplace=True)\n",
    "reduced_df['purpose'].replace('wedding','major_purchase',inplace=True)\n",
    "reduced_df['purpose'].replace('vacation','major_purchase',inplace=True)\n",
    "reduced_df['purpose'].replace('moving','house',inplace=True)\n",
    "reduced_df['purpose'].replace('home_improvement ','house',inplace=True)\n",
    "reduced_df['purpose'].replace('renewable_energy ','house',inplace=True)\n",
    "\n",
    "reduced_df=pd.get_dummies(data=reduced_df,columns=['application_type','term',\n",
    "                                                   'verification_status','purpose'],\n",
    "                          drop_first=True)\n",
    "\n",
    "# 8/8 removed 'home_ownership\n",
    "reduced_df['home_ownership'] = np.where(reduced_df['home_ownership']=='OWN',1,0)\n",
    "\n",
    "setb=set(reduced_df.columns)\n",
    "print(setb-seta)\n",
    "print(reduced_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to treat NaN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's remove majority NaN columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 annual_inc_joint 9529\n",
      "1 dti_joint 9529\n",
      "2 mths_since_last_record 172185\n",
      "3 mths_since_recent_bc_dlq 240759\n",
      "4 mths_since_last_major_derog 264308\n",
      "5 mths_since_rcnt_il 289098\n",
      "6 all_util 296067\n",
      "7 inq_last_12m 296092\n",
      "8 open_acc_6m 296092\n",
      "9 total_cu_tl 296092\n",
      "10 inq_fi 296093\n",
      "11 open_il_12m 296093\n",
      "12 open_il_24m 296093\n",
      "13 open_rv_12m 296093\n",
      "14 open_rv_24m 296093\n",
      "15 mths_since_recent_revol_delinq 338379\n",
      "16 mths_since_last_delinq 503506\n",
      "17 mths_since_recent_inq 881746\n",
      "18 mo_sin_old_il_acct 924972\n",
      "19 mo_sin_old_rev_tl_op 953024\n",
      "20 mo_sin_rcnt_rev_tl_op 953024\n",
      "21 num_rev_accts 953024\n",
      "22 mo_sin_rcnt_tl 953025\n",
      "23 num_accts_ever_120_pd 953025\n",
      "24 num_bc_tl 953025\n"
     ]
    }
   ],
   "source": [
    "has_data = {}\n",
    "for column in reduced_df.columns:\n",
    "    has_data[column] = len(reduced_df[column].dropna())\n",
    "has_data\n",
    "\n",
    "order_has_data=sorted(has_data, key=lambda dict_key: has_data[dict_key])\n",
    "\n",
    "top_sparse=25\n",
    "for i,j in zip(range(top_sparse),order_has_data[0:top_sparse]):\n",
    "    print(i,j, has_data[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(882558, 58)\n"
     ]
    }
   ],
   "source": [
    "nonnan_df=reduced_df.drop(columns=order_has_data[0:19]).dropna()\n",
    "nonnan_df = nonnan_df[nonnan_df['duration']!=0]\n",
    "print(nonnan_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.018723e+06\n",
       "mean     2.010069e+01\n",
       "std      1.230952e+01\n",
       "min     -0.000000e+00\n",
       "25%      1.013333e+01\n",
       "50%      1.830000e+01\n",
       "75%      3.043333e+01\n",
       "max      7.100000e+01\n",
       "Name: duration, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df['duration'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction Variables\n",
    "I added anything that an amount as an interaction variable compared with income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_vars = ['dti','inq_last_6mths']\n",
    "\n",
    "for col in poly_vars:\n",
    "    new_title = col + '_2'\n",
    "    nonnan_df[new_title] = nonnan_df[col] * nonnan_df[col]\n",
    "    new_title = col + '_3'\n",
    "    nonnan_df[new_title] = nonnan_df[col] * nonnan_df[col] * nonnan_df[col]\n",
    "for col in ['revol_util','tot_coll_amt','loan_amnt','installment']:\n",
    "    new_title = col + '_over_inc'\n",
    "    nonnan_df[new_title] = nonnan_df[col] / nonnan_df['annual_inc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's work with training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape,nonnan_df.shape)\n",
    "\n",
    "# need to look at interest rate as well!!!\n",
    "y=nonnan_df['total_pymnt']/(nonnan_df['duration'])\n",
    "\n",
    "traintest_df=nonnan_df.drop(columns=['total_pymnt','duration'])\n",
    "traintest_df['issue_d'] = traintest_df['issue_d'].dt.year\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    traintest_df,y,test_size=0.2,random_state=42)#,stratify=nonnan_df[['loan_status']])\n",
    "\n",
    "Xscaler = StandardScaler()\n",
    "Xscaler.fit_transform(X_train,X_test)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCOLSModel = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print(LCOLSModel.intercept_)\n",
    "print(LCOLSModel.coef_)\n",
    "\n",
    "# your code here\n",
    "trainR2 = r2_score(y_train,LCOLSModel.predict(X_train))\n",
    "\n",
    "testR2 = r2_score(y_test,LCOLSModel.predict(X_test))\n",
    "\n",
    "print(\"The training set OLS regression R^2 score is: %f\" % trainR2)\n",
    "print(\"The test set OLS regression R^2 score is: %f\" % testR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [.001, .005, 1, 5, 10, 50, 100, 500, 1000]\n",
    "\n",
    "kfold = KFold(5, shuffle=True) # use this for testing\n",
    "\n",
    "LCRRModel = RidgeCV(alphas=lambdas, cv=kfold)\n",
    "LCRRModel.fit(X_train, y_train)\n",
    "LCRR_shrinkage_parameter=LCRRModel.alpha_\n",
    "\n",
    "print(\"Best model searched:\\nalpha = {}\\nintercept = {}\\nbetas = {}, \".format(LCRR_shrinkage_parameter,\n",
    "                                                                            LCRRModel.intercept_,\n",
    "                                                                            LCRRModel.coef_\n",
    "                                                                            )\n",
    "     )\n",
    "\n",
    "RRtrainR2 = r2_score(y_train,LCRRModel.predict(X_train))\n",
    "RRtestR2 = r2_score(y_test,LCRRModel.predict(X_test))\n",
    "print(\"The training set Ridge regression R^2 score is: %f\" % RRtrainR2)\n",
    "print(\"The test set Ridge regression R^2 score is: %f\" % RRtestR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLRModel = LassoCV(alphas=lambdas, cv=kfold)\n",
    "LCLRModel.fit(X_train, y_train)\n",
    "LCLR_shrinkage_parameter = LCLRModel.alpha_\n",
    "print(\"Best model searched:\\nalpha = {}\\nintercept = {}\\nbetas = {}, \".format(LCLR_shrinkage_parameter,\n",
    "                                                                            LCLRModel.intercept_,\n",
    "                                                                            LCLRModel.coef_\n",
    "                                                                            )\n",
    "     )\n",
    "\n",
    "LRtrainR2 = r2_score(y_train,LCLRModel.predict(X_train))\n",
    "LRtestR2 = r2_score(y_test,LCLRModel.predict(X_test))\n",
    "print(\"The training set Lasso regression R^2 score is: %f\" % LRtrainR2)\n",
    "print(\"The test set Lasso regression R^2 score is: %f\" % LRtestR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df=pd.DataFrame(np.array([X_train.columns,\n",
    "                               LCOLSModel.coef_,\n",
    "                               LCRRModel.coef_,\n",
    "                               LCLRModel.coef_]).T,columns=[\"feature\",\"OLS\",\"RR\",\"LR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(coef_df[\"feature\"],[-abs(x) for x in coef_df[\"OLS\"].values],label='OLS',alpha=1.0)\n",
    "plt.bar(coef_df[\"feature\"],-abs(coef_df[\"RR\"]),label='RR',alpha=0.5)\n",
    "plt.bar(coef_df[\"feature\"],abs(coef_df[\"LR\"]),label='LR',alpha=0.5)\n",
    "plt.yscale(\"linear\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df[abs(coef_df['OLS'])>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Models\n",
    "We continue to have problems with multicolinearity which may be contributing to some of the problems we are seeing in the data. I am going to try additional models that may be more suitable.\n",
    "\n",
    "Because of the sheer size of our dataset, the following block of code take a long time to run. I ran one with full datasets without much luck. To reduce the runtime, I pulled a random sample from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9cbedc83f9e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msample_percent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.05\u001b[0m \u001b[1;31m# I put this ridiculouly low to ensure code would compile. I will run again overnight with a\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# larger number for additional insights.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msample_percent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_train_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train_sample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "sample_percent = .05 # I put this ridiculouly low to ensure code would compile. I will run again overnight with a \n",
    "# larger number for additional insights.\n",
    "X_train_sample = X_train.reset_index(drop=True).sample(int(len(X_train)*sample_percent),random_state=42)\n",
    "y_train_sample = y_train.reset_index(drop=True).iloc[X_train_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neu_net = Sequential([\n",
    "    Dense(800, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(800, activation='relu'),\n",
    "    Dense(400, activation='relu'),\n",
    "    Dense(400, activation='relu'),\n",
    "    Dense(400, activation='relu'),\n",
    "    Dense(1, activation='relu')\n",
    "])\n",
    "neu_net.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "neu_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neu_net.fit(X_train_sample,y_train_sample, epochs=20, batch_size=64, validation_split = .25)\n",
    "# I was not seeing improvement between epochs. This is not sutiable data for neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't see an improvement in the loss function. I'm going to try alternative methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RandomForestRegressor(10)\n",
    "RF_model.fit(X_train_sample,y_train_sample)\n",
    "RF_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = []\n",
    "avg_score = []\n",
    "sd_score = []\n",
    "for i in range(5,30):\n",
    "    s = i * 4\n",
    "    size += [s]\n",
    "    RF_model = RandomForestRegressor(s)\n",
    "    cv = cross_val_score(RF_model,X_train_sample,y_train_sample,cv=6,n_jobs=4)\n",
    "    avg_score +=  [np.average(cv)]\n",
    "    sd_score += [np.std(cv)]\n",
    "    print(i,'complete')\n",
    "    print(avg_score,'avg_score')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(size,avg_score)\n",
    "bound = []\n",
    "for i in range(len(size)):\n",
    "    bound += [avg_score[i]+sd_score[i]]\n",
    "plt.plot(size,bound,c='orange')\n",
    "\n",
    "bound = []\n",
    "for i in range(len(size)):\n",
    "    bound += [avg_score[i]-sd_score[i]]\n",
    "plt.plot(size,bound,c='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
