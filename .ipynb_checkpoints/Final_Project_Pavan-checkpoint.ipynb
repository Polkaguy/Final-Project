{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#import statsmodels.api as sm,\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from pandas.core import datetools\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "The following code imports and validates the LendingClub data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./Source Data/Loan Data/LoanStats3a_securev1.csv\n",
      "Reading ./Source Data/Loan Data/LoanStats3b_securev1.csv\n",
      "Reading ./Source Data/Loan Data/LoanStats3c_securev1.csv\n",
      "Reading ./Source Data/Loan Data/LoanStats3d_securev1.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports loan data\n",
    "\n",
    "df = pd.DataFrame()\n",
    "basepath='./Source Data/Loan Data/'\n",
    "files = os.listdir(basepath)\n",
    "csvs = []\n",
    "\n",
    "for file in files:\n",
    "    if re.match('.*csv$',file):\n",
    "        csvs += [file]\n",
    "\n",
    "try:\n",
    "    df=pickle.load(open('full_data.p', 'rb'))\n",
    "    print('Read from pickle')\n",
    "except:\n",
    "    dates = [\n",
    "        'next_pymnt_d',\n",
    "        'hardship_start_date',\n",
    "        'hardship_end_date',\n",
    "        'payment_plan_start_date',\n",
    "        'earliest_cr_line',\n",
    "        'issue_d',\n",
    "        'debt_settlement_flag_date',\n",
    "        'last_pymnt_d'\n",
    "\n",
    "    ]\n",
    "    cols = df.dtypes\n",
    "    for csv in csvs:\n",
    "        path = basepath + csv\n",
    "        print(\"Reading\",path)\n",
    "        tdf = pd.read_csv(path,header=1,parse_dates=dates,low_memory=False)\n",
    "        df=df.append(tdf)\n",
    "    df.reset_index(inplace=True) # This will help with joining back data if necessary.\n",
    "    # Convert dates to datetime\n",
    "    #df['issue_d'] = pd.to_datetime(df['issue_d'])\n",
    "    #df['debt_settlement_flag_date'] = pd.to_datetime(df['debt_settlement_flag_date'])\n",
    "    df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'])\n",
    "    \n",
    "    #determine age of credit line prior to loan issue and convert to integer\n",
    "    # days of credit history\n",
    "    df['earliest_cr_line'] = (df['issue_d']-df['earliest_cr_line']).dt.days\n",
    "    \n",
    "    # convert issue_d to a year to consider economic conditions\n",
    "    #SHOULD WE GO TO QUARTERS?\n",
    "    #df['issue_d'] = df['issue_d'].dt.year\n",
    "    \n",
    "    df['duration'] = (df['last_pymnt_d'] - df['issue_d']).dt.days / 30\n",
    "    pickle.dump( df, open( \"full_data.p\", \"wb\" ) )\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200)\n",
    "print(df.dtypes)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for duplicates within our combined data\n",
    "df['count']=1 # Generate field to measure the number of occurances\n",
    "\n",
    "counts = df.groupby('id')['count'].count()\n",
    "print('Data are duplicated %i times.' % len(counts[counts>1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are duplicated 0 times.\n",
    "\n",
    "Some data are not useful as a predictor. Because we are going to use these data as a training set, we need to remove data that are not useful for predictions. There are two categories of data that are not useful for predictions. First, we need information about loans that have fully run their course. That is to say, we need to see loans that have either been paid in full or written off. It is not useful to see loans that are currently delinquent or current on payments but still early in the loan. Second, we need to remove predictors that are not import for predicting the loan. For example the URL has no impact on the borrower's ability to repay the loan. If we need this data in the future, we can still pair it back with the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_data = {}\n",
    "for column in df.columns:\n",
    "    has_data[column] = len(df[column].dropna())\n",
    "#print(len(has_data))\n",
    "#has_data\n",
    "\n",
    "order_has_data=sorted(has_data, key=lambda dict_key: has_data[dict_key])\n",
    "\n",
    "top_sparse=50\n",
    "for i,j in zip(order_has_data[0:top_sparse],range(top_sparse)):\n",
    "    print(j, i, has_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data, it appears that joint applications are a fairly new development in these data. We see only about 50,000 instances where the data are included for joint applications. The following visualization shows the relationship for income in joint applicants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_fil = (df['annual_inc']>0)&(df['annual_inc_joint']>0)\n",
    "plt.scatter(df[joint_fil]['annual_inc'],df[joint_fil]['annual_inc_joint'])\n",
    "plt.title('Income vs. Joint Income')\n",
    "plt.xlabel('Annual income')\n",
    "plt.ylabel('Annual income joint')\n",
    "plt.show;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the high risk of multicolinearity, and the small number of observations, we are dropping this from the dataset. We will still keep a dummy variable that treats joint applications differently, but we will not consider the joint income of applicants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates to datetime\n",
    "df['issue_d'] = pd.to_datetime(df['issue_d'])\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'])\n",
    "\n",
    "#determine age of credit line prior to loan issue and convert to integer\n",
    "# days of credit history\n",
    "df['earliest_cr_line'] = (df['issue_d']-df['earliest_cr_line']).dt.days\n",
    "\n",
    "# convert issue_d to a year to consider economic conditions\n",
    "#SHOULD WE GO TO QUARTERS?\n",
    "df['issue_d'] = df['issue_d'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to loans that are paid in full or written off. Uses dates so that \n",
    "# loans that are delinquent are not disproportionaltely dropped from data\n",
    "\n",
    "mature_filter = (df['loan_status']=='Fully Paid')|(df['loan_status']=='Charged Off')\n",
    "#latest_mature = df[~mature_filter]['issue_d'].min()\n",
    "#latest_mature\n",
    "#reduced_df = df[df['issue_d']<=latest_mature]\n",
    "reduced_df = df[mature_filter] # Pulls only loans that are charged off or paid in full.\n",
    "#\n",
    "## Use my documentation to filter to only \n",
    "data_dict = pd.read_excel('./Source Data/LCDataDictionary.xlsx',sheet_name='LoanStats')\n",
    "features = list(data_dict[data_dict['Useful Predictor']=='Yes']['LoanStatNew'].values)\n",
    "reduced_df=reduced_df[features]\n",
    "\n",
    "# Combines fields when necessary\n",
    "reduced_df['fico_est'] = (reduced_df['fico_range_low']+reduced_df['fico_range_high'])/2\n",
    "\n",
    "reduced_df.drop(columns=['fico_range_low','fico_range_high'],inplace=True)\n",
    "print(reduced_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup our df\n",
    "backup_df = reduced_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restore our df\n",
    "reduced_df = backup_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to numbers emp_length, int_rate, revol_util\n",
    "emp_length_map={'10+ years':10, '< 1 year':0, '1 year':1, '3 years':3, '8 years':8, '9 years':9,\n",
    "                '4 years':4, '5 years':5, '6 years':6, '2 years':2, '7 years':7}\n",
    "\n",
    "reduced_df['emp_length']=reduced_df['emp_length'].replace(pd.Series(emp_length_map))\n",
    "\n",
    "grade_map={\"A\":1,\"B\":2,\"C\":3,\"D\":4,\"E\":5,\"F\":6,\"G\":7}\n",
    "reduced_df['grade']=reduced_df['grade'].replace(pd.Series(grade_map))\n",
    "\n",
    "reduced_df['int_rate']=reduced_df['int_rate'].apply(lambda x: float(x[:-1]))\n",
    "reduced_df['revol_util']=reduced_df['revol_util'].apply(lambda x:\n",
    "                                                        x[:-1] if isinstance(x, str) else np.nan).astype(float)\n",
    "\n",
    "reduced_df['earliest_cr_line']=reduced_df['earliest_cr_line'].apply(lambda x:\n",
    "                                                        0.0 if np.isnan(x) else x)\n",
    "\n",
    "reduced_df.drop(columns=['emp_title'],inplace=True)\n",
    "\n",
    "print(reduced_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which values are categorical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_options = {}\n",
    "for column in reduced_df.columns:\n",
    "    n_options[column] = len(reduced_df[column].unique())\n",
    "#n_options\n",
    "order_n_options=sorted(n_options, key=lambda dict_key: n_options[dict_key])\n",
    "\n",
    "for i in order_n_options[0:50]:\n",
    "    print(i, n_options[i],reduced_df[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, we will have to encode several data as dummy variables:\n",
    "\n",
    "- application_type\n",
    "- term\n",
    "- verification_status\n",
    "- home_ownership\n",
    "- purpose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Multicolinearity Analysis\n",
    "I performed some additional analysis on multicolinearity. Some of the dummy variables were over 7,000. The reason being is the one hot encoding is dropping the first thing it encounters. In some cases there are only 1 or two instances of it. Therefore, there is some repetition in dummy variables. As a solution, I tried using only one of the possible answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in reduced_df['purpose'].unique():\n",
    "    print(p, len(df[df['purpose']==p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated where logical\n",
    "reduced_df['purpose'].replace('credit_card','debt_consolidation',inplace=True)\n",
    "reduced_df['purpose'].replace('educational','other',inplace=True)\n",
    "reduced_df['purpose'].replace('wedding','major_purchase',inplace=True)\n",
    "reduced_df['purpose'].replace('vacation','major_purchase',inplace=True)\n",
    "reduced_df['purpose'].replace('moving','house',inplace=True)\n",
    "reduced_df['purpose'].replace('home_improvement ','house',inplace=True)\n",
    "reduced_df['purpose'].replace('renewable_energy ','house',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seta=set(reduced_df.columns)\n",
    "\n",
    "\n",
    "# 8/8 consolidated purpose\n",
    "\n",
    "reduced_df=pd.get_dummies(data=reduced_df,columns=['application_type','term',\n",
    "                                                   'verification_status','purpose'],\n",
    "                          drop_first=True)\n",
    "\n",
    "# 8/8 removed 'home_ownership\n",
    "reduced_df['home_ownership'] = np.where(reduced_df['home_ownership']=='OWN',1,0)\n",
    "\n",
    "setb=set(reduced_df.columns)\n",
    "print(setb-seta)\n",
    "print(reduced_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to treat NaN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's remove majority NaN columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_data = {}\n",
    "for column in reduced_df.columns:\n",
    "    has_data[column] = len(reduced_df[column].dropna())\n",
    "has_data\n",
    "\n",
    "order_has_data=sorted(has_data, key=lambda dict_key: has_data[dict_key])\n",
    "\n",
    "top_sparse=25\n",
    "for i,j in zip(range(top_sparse),order_has_data[0:top_sparse]):\n",
    "    print(i,j, has_data[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnan_df=reduced_df.drop(columns=order_has_data[0:19])\n",
    "print(nonnan_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reduced_df.dropna()),len(nonnan_df.dropna()))\n",
    "nonnan_df=nonnan_df.dropna()\n",
    "print(nonnan_df.shape)\n",
    "\n",
    "n_options = {}\n",
    "for column in nonnan_df.columns:\n",
    "    n_options[column] = len(nonnan_df[column].unique())\n",
    "#n_options\n",
    "order_n_options=sorted(n_options, key=lambda dict_key: n_options[dict_key])\n",
    "\n",
    "for i in order_n_options[0:49]:\n",
    "    print(i, n_options[i],nonnan_df[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What factors are correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlatable columns\n",
    "corr_columns = order_n_options[24:]\n",
    "print(nonnan_df[corr_columns].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df=nonnan_df[corr_columns]\n",
    "\n",
    "matrix=np.corrcoef(corr_df,rowvar=False)-np.eye(len(corr_df.columns))\n",
    "\n",
    "\n",
    "i,j=np.nonzero(abs(matrix) > 0.7)\n",
    "print(\"Factors with high correlation (> +/-0.7) are:\")\n",
    "for k in range(len(i)):\n",
    "    print(\"\\t\",k,corr_df.columns[i[k]],\"vs\",corr_df.columns[j[k]],\"=\",matrix[i[k]][j[k]])\n",
    "    \n",
    "\n",
    "# cube each value to highlight higher correlation elements\n",
    "matrix=abs(matrix**3)\n",
    "plt.figure(figsize=(13,10))\n",
    "plt.pcolor(matrix,cmap='brg')\n",
    "\n",
    "plt.xticks(range(len(corr_df.columns)), corr_df.columns,rotation='vertical')\n",
    "plt.yticks(range(len(corr_df.columns)), corr_df.columns)\n",
    "plt.colorbar(cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing redundancy\n",
    "We see that some of the factors in this dataset are two ways of measuring the same thing. For example, interest rates (`int_rate`) and the grading of the loan (`grade`) two ways of saying the same thing. A lower grade of loan indicates higher risk and higher risk is offset with a higher interest rate. Indeed, we see a large amount of correlation between the two fields.\n",
    "\n",
    "As another example, we can see the percentage of bankcard accounts > 75% of the limit (`percent_bc_gt_75`) is highly correlated with the utilization of bank cards(`revol_bal`). Both of these are similar to the maximum amount of credit utilization (`total_rev_hi_lim`).\n",
    "\n",
    "For this reason we need to select variables with high correlation and choose the variables that will have the biggest impact on our model. I have created the following groups that will need to be consolidated.\n",
    "\n",
    "## High-Magnitude Factors\n",
    "The following code regularizes the data and then looks for the regressor that, when considered independently has the higest impact on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standardized dataframe\n",
    "std_df = nonnan_df.copy()\n",
    "y=nonnan_df['total_pymnt']/nonnan_df['loan_amnt']\n",
    "for col in std_df.columns:\n",
    "    std_df[col] = (std_df[col] - std_df[col].min()) / (std_df[col].max() - std_df[col].min())\n",
    "x_full=std_df.drop(columns=['loan_amnt','loan_amnt'])\n",
    "# Revolving debt maximum related variables\n",
    "revolving = [\n",
    "    'bc_open_to_buy',\n",
    "    'num_rev_accts',\n",
    "    'num_rev_tl_bal_gt_0',\n",
    "    'revol_bal',\n",
    "    'revol_util',\n",
    "    'total_rev_hi_lim',\n",
    "    'tot_hi_cred_lim',\n",
    "    'num_il_tl',\n",
    "    'open_acc',\n",
    "    'num_bc_tl',\n",
    "    'num_sats',\n",
    "    'num_op_rev_tl',\n",
    "    'num_bc_sats',\n",
    "    'mo_sin_old_rev_tl_op',\n",
    "    'total_il_high_credit_limit'\n",
    "]\n",
    "\n",
    "test_df = pd.DataFrame(columns=['Predictor','ABS Coef','R2'])\n",
    "for col in revolving:\n",
    "    cols_to_drop = revolving.copy()\n",
    "    cols_to_drop.remove(col)\n",
    "    x=x_full.drop(columns=cols_to_drop)\n",
    "    test_reg = LinearRegression().fit(x,y)\n",
    "    test_coef = test_reg.coef_[list(x.columns).index(col)]\n",
    "    test_score = test_reg.score(x,y)\n",
    "    test_df = test_df.append({\n",
    "        'Predictor':col,\n",
    "        'ABS Coef':abs(test_coef),\n",
    "        'R2':test_score\n",
    "    },ignore_index=True)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By far, total Revolving Debt limit has the biggest impact in terms of magnitude. We should keep this as the one regressor to represent revolving debt. This will help us to eliminate multicolinearity. Let's explore the impact this has on our correlation matrix\n",
    "\n",
    "Note:\n",
    "I did not perform the same analysis for the grade and interest rate. The loans are structured so that the grade determines the range of interest rates. The steps within the grade specifically determine the interest rate. Therefore, the grade the step and the rate all give the same information, but the rate gives the specific detail.\n",
    "\n",
    "Note: In the new dataset, I replace the payment amount with the percentage of income that is dedicated to the payment. We can have all the factors to calculate loan amount (term, rate, and original balance) as individual regressors. Replacing it will still capture a critical element of the loan amount while removing multicolinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduc_corr_columns = corr_columns.copy()\n",
    "\n",
    "#reduc_corr_columns[]\n",
    "to_remove = revolving.copy()\n",
    "to_remove += ['grade'] # Extra code to drop grade\n",
    "\n",
    "to_remove.remove('total_rev_hi_lim')\n",
    "for x in to_remove:\n",
    "    try:\n",
    "        reduc_corr_columns.remove(x)\n",
    "    except:\n",
    "        print(\"error removing\",x)\n",
    "\n",
    "corr_df=nonnan_df[reduc_corr_columns]\n",
    "\n",
    "matrix=np.corrcoef(corr_df,rowvar=False)-np.eye(len(corr_df.columns))\n",
    "\n",
    "\n",
    "i,j=np.nonzero(abs(matrix) > 0.7)\n",
    "print(\"Factors with high correlation (> +/-0.7) are:\")\n",
    "for k in range(len(i)):\n",
    "    print(\"\\t\",k,corr_df.columns[i[k]],\"vs\",corr_df.columns[j[k]],\"=\",matrix[i[k]][j[k]])\n",
    "    \n",
    "\n",
    "# cube each value to highlight higher correlation elements\n",
    "matrix=abs(matrix**3)\n",
    "plt.figure(figsize=(13,10))\n",
    "plt.pcolor(matrix,cmap='brg')\n",
    "\n",
    "plt.xticks(range(len(corr_df.columns)), corr_df.columns,rotation='vertical')\n",
    "plt.yticks(range(len(corr_df.columns)), corr_df.columns)\n",
    "plt.colorbar(cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- We still see some multicolinearity with the loan payment section. I plan to address additional factors in the factoring for income section.\n",
    "- Total payments is one of the planned Y variables. For instances where it has high correlation, we expect to see strong relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factoring for income\n",
    "The correlation matrix emphasizes some factors in the data are highly correlated. In some instances, this correlation may be due to the fact that the predictors are dependent on other predictors. For example, we can consider three predictors in concert: the income, the loan repayment and the interest rate assigned by Lending Club. We are assuming that a higher amount of risk is related to a higher interest rate. That is to say, Lending Club assesses that loans are riskier based on loan to income ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the relationship between loan repayment and income:\n",
    "features = ['annual_inc','installment','int_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large outliers make the trend difficult to understand so I'm making a trimmed df\n",
    "temp_df = nonnan_df[nonnan_df['annual_inc']<150000][['installment','annual_inc']]\n",
    "x = temp_df['annual_inc']\n",
    "y = temp_df['installment']\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.hexbin(x,y,gridsize =40)\n",
    "plt.xlabel('Annual Income ($)')\n",
    "plt.ylabel('Loan Installment Amount ($)')\n",
    "plt.title('Income vs. Payment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it looks like income and loan amount are both approximately normally distributed, we see that there is a clear fan pattern here. No monthly loan payment is larger than approximately 1% of annual income. We may be able to keep valuable information from both datasets and reduce multicolinearity by considering not the individuals variables, but the relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnan_df['percent_of_income'] = nonnan_df['installment']*12/nonnan_df['annual_inc']\n",
    "nonnan_df['percent_of_income'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we see some odd cases, I looked up an extreme and found that we see odd values when considering joint incomes. These instances are rare (fewer than 40 cases where total payments are greater than total income). This loan was funded as a joint application. As a result we are going to have to throw out any data about joint applications as it only represents 8000 cases, but creates significant leverage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing an extreme example from the\n",
    "df.iloc[1542746]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional factors to remove\n",
    "if 'installment' not in to_remove:\n",
    "    to_remove += ['installment']\n",
    "    to_remove += ['total_pymnt','loan_amnt']\n",
    "    #to_remove += ['issue_d','fico_est'] This is where we hit diminishing returns with multicolinearity.\n",
    "    # Our VIFs are still astronomically high, but we will have to resolve in other ways.\n",
    "    \n",
    "    \n",
    "features = list(set(nonnan_df.columns) - set(to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Multicolinearity Analysis\n",
    "Based on the poor performance of the models. I performed additional analysis to determine the impact of multicolinearity on our data. Note: Based on this analysis, I simplified some of the dummy variables in a later model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.columns.shape[0])]\n",
    "vif['Feature'] = X_train.columns\n",
    "vif.sort_values('VIF',ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our multicolinearity is much improved from previous models, but still unacceptably high. Issue date and FICO still have high multicoliearity. Although we did not test it as part of this project, I hypothesize that the quality of applicants change as  Lending Club gets additional borrowers on the platform and more discerning lenders.\n",
    "\n",
    "FICO has high multicoliearity because there is enough information to calculate the FICO score from teh test set. Because of how FICO is weighted most heavily by on time payments (Source: Fair Issac Co. https://www.myfico.com/credit-education/whats-in-your-credit-score/). Therefore, the FICO is related to the other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corval = []\n",
    "for i in X_train.columns:\n",
    "    if not i == 'issue_d':\n",
    "        print(i, np.corrcoef(X_train['issue_d'],X_train[i])[0][1])\n",
    "        corval += [np.corrcoef(X_train['issue_d'],X_train[i])[0][1]]\n",
    "print(max(corval))\n",
    "\n",
    "# Multicolinearity is high but there are no single factors contributing\n",
    "# Simply put, the quality of lenders changed as the program progressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most extreme coefficient of correlation with year is the `percent_bc_gt_75`. This relationship is not meaningful as these describe different things. `percent_bc_gt_75` describes the utilization of bank cards. Additionally, the score is sufficiently low that it, alone is not the cause of the multicolienarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat map Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the loan status versus purpose\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "purp_loan= ['purpose', 'loan_status']\n",
    "cm = sns.light_palette(\"red\", as_cmap=True)\n",
    "pd.crosstab(df[purp_loan[0]], df[purp_loan[1]]).style.background_gradient(cmap = cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the loan status versus loan grade\n",
    "loan_grade = ['loan_status', 'grade']\n",
    "cm = sns.light_palette(\"red\", as_cmap=True)\n",
    "pd.crosstab(df[loan_grade[0]], df[loan_grade[1]]).style.background_gradient(cmap = cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the loan status versus home ownership\n",
    "loan_home = ['loan_status', 'home_ownership']\n",
    "cm = sns.light_palette(\"red\", as_cmap=True)\n",
    "pd.crosstab(df[loan_home[0]], df[loan_home[1]]).style.background_gradient(cmap = cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the loan_status versus loan application_type\n",
    "loan_application = ['loan_status', 'application_type']\n",
    "cm = sns.light_palette(\"red\", as_cmap=True)\n",
    "pd.crosstab(df[loan_application[0]], df[loan_application[1]]).style.background_gradient(cmap = cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the State Address versus Loan Status\n",
    "adress_loan = ['addr_state', 'loan_status']\n",
    "cm = sns.light_palette(\"red\", as_cmap=True)\n",
    "pd.crosstab(df[adress_loan[0]], df[adress_loan[1]]).style.background_gradient(cmap = cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore distrubution of loan interest rates\n",
    "plt.figure(figsize = (15,12))\n",
    "plt.subplot(211)\n",
    "g1 = sns.countplot(x=\"int_rate\",data=df, \n",
    "                   palette=\"Set2\")\n",
    "g1.set_xlabel(\"Loan Interest Rate\", fontsize=12)\n",
    "g1.set_ylabel(\"Count\", fontsize=12)\n",
    "g1.set_title(\"Distribuition of Loan Interest Rates\", fontsize=20)\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.4, hspace = 1.2,top = 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "\n",
    "g = sns.violinplot(x=\"home_ownership\",y=\"loan_amnt\",data=df,\n",
    "               kind=\"violin\",\n",
    "               split=True,palette=\"hls\",\n",
    "               hue=\"application_type\")\n",
    "g.set_title(\"Homer Ownership - Loan Distribuition\", fontsize=20)\n",
    "g.set_xlabel(\"\", fontsize=15)\n",
    "g.set_ylabel(\"Loan Amount\", fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Loan Purpose\n",
    "print(\"Purposes count description: \")\n",
    "print(pd.crosstab(df.purpose, df.application_type))\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "plt.subplot(211)\n",
    "g = sns.countplot(x=\"purpose\",data=df,\n",
    "                  palette='hls')\n",
    "g.set_xticklabels(g.get_xticklabels(),rotation=45)\n",
    "g.set_title(\"Application Type - Loan Amount\", fontsize=20)\n",
    "g.set_xlabel(\"\", fontsize=15)\n",
    "g.set_ylabel(\"Loan Amount\", fontsize=15)\n",
    "\n",
    "plt.subplot(212)\n",
    "g1 = sns.violinplot(x=\"purpose\",y=\"loan_amnt\",data=df,\n",
    "               hue=\"application_type\", split=True)\n",
    "g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n",
    "g1.set_title(\"Application Type - Loan Amount\", fontsize=20)\n",
    "g1.set_xlabel(\"\", fontsize=15)\n",
    "g1.set_ylabel(\"Loan Amount\", fontsize=15)\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.2, hspace = 0.8,top = 0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "\n",
    "g = sns.violinplot(x='loan_status', y=\"installment\",\n",
    "                   data=df)\n",
    "g.set_xticklabels(g.get_xticklabels(),rotation=45)\n",
    "g.set_xlabel(\"\", fontsize=12)\n",
    "g.set_ylabel(\"Installment Distrubution\", fontsize=15)\n",
    "g.set_title(\"Loan Status by Installment\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.loan_status == \\\n",
    "            'Does not meet the credit policy. Status:Fully Paid', 'loan_status'] = 'NMCP Fully Paid'\n",
    "df.loc[df.loan_status == \\\n",
    "            'Does not meet the credit policy. Status:Charged Off', 'loan_status'] = 'NMCP Charged Off'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loan_status.value_counts())\n",
    "\n",
    "plt.figure(figsize = (18,21))\n",
    "\n",
    "plt.subplot(311)\n",
    "g = sns.countplot(x=\"loan_status\", data=df)\n",
    "g.set_xticklabels(g.get_xticklabels(),rotation=45)\n",
    "g.set_xlabel(\"\", fontsize=12)\n",
    "g.set_ylabel(\"Count\", fontsize=15)\n",
    "g.set_title(\"Loan Status Count\", fontsize=20)\n",
    "\n",
    "plt.subplot(312)\n",
    "g1 = sns.boxplot(x=\"loan_status\", y=\"total_acc\", data=df)\n",
    "g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n",
    "g1.set_xlabel(\"\", fontsize=12)\n",
    "g1.set_ylabel(\"Total Acc\", fontsize=15)\n",
    "g1.set_title(\"Duration Count\", fontsize=20)\n",
    "\n",
    "plt.subplot(313)\n",
    "g2 = sns.violinplot(x=\"loan_status\", y=\"loan_amnt\", data=df)\n",
    "g2.set_xticklabels(g2.get_xticklabels(),rotation=45)\n",
    "g2.set_xlabel(\"Duration Distribuition\", fontsize=15)\n",
    "g2.set_ylabel(\"Count\", fontsize=15)\n",
    "g2.set_title(\"Loan Amount\", fontsize=20)\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.2, hspace = 0.7,top = 0.9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
